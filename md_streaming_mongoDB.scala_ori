import org.apache.spark.SparkContext
import org.apache.spark.SparkConf
import _root_.kafka.serializer.StringDecoder
import org.apache.spark.streaming._
import org.apache.spark.streaming.kafka._
import org.apache.spark.streaming.kafka.KafkaUtils
import org.apache.spark.sql.hive.HiveContext
import java.util.Calendar
import org.apache.phoenix.spark._
import org.apache.spark.sql.SparkSession

import org.apache.spark._
import org.apache.spark.rdd.NewHadoopRDD
import org.apache.hadoop.hbase.{HBaseConfiguration, HTableDescriptor}
import org.apache.hadoop.hbase.client.HBaseAdmin
import org.apache.hadoop.hbase.mapreduce.TableInputFormat
import org.apache.hadoop.fs.Path
import scala.util.Random
import scala.math._
import org.apache.spark.sql.functions._
//
import com.mongodb.spark._
import org.bson.Document
import com.mongodb.spark.config._
//
import org.apache.log4j.Logger
import org.apache.log4j.Level


//
object md_streaming_mongoDB
{
  private var zookeeperUrl = "rhes75:2181"
  private var requestConsumerId = null
  private var impressionConsumerId = null
  private var clickConsumerId = null
  private var conversionConsumerId = null
  private var requestTopicName = null
  private var impressionTopicName = null
  private var clickTopicName = null
  private var conversionTopicName = null
  private var requestThreads = 0
  private var impressionThreads = 0
  private var clickThreads = 0
  private var conversionThreads = 0
  private var sparkAppName = "md_streaming_mongoDB"
  private var sparkMasterUrl = "local[12]"
  private var sparkDefaultParllelism = null
  private var sparkDefaultParallelismValue = "12"
  private var sparkSerializer = null
  private var sparkSerializerValue = "org.apache.spark.serializer.KryoSerializer"
  private var sparkNetworkTimeOut = null
  private var sparkNetworkTimeOutValue = "3600"
  private var sparkStreamingUiRetainedBatches = null
  private var sparkStreamingUiRetainedBatchesValue = "5"
  private var sparkWorkerUiRetainedDrivers = null
  private var sparkWorkerUiRetainedDriversValue = "5"
  private var sparkWorkerUiRetainedExecutors = null
  private var sparkWorkerUiRetainedExecutorsValue = "30"
  private var sparkWorkerUiRetainedStages = null
  private var sparkWorkerUiRetainedStagesValue = "100"
  private var sparkUiRetainedJobs = null
  private var sparkUiRetainedJobsValue = "100"
  private var sparkJavaStreamingDurationsInSeconds = "10"
  private var sparkNumberOfSlaves = 14
  private var sparkRequestTopicShortName = null
  private var sparkImpressionTopicShortName = null
  private var sparkClickTopicShortName = null
  private var sparkConversionTopicShortName = null
  private var sparkNumberOfPartitions = 30
  private var sparkClusterDbIp = null
  private var clusterDbPort = null
  private var insertQuery = null
  private var insertOnDuplicateQuery = null
  private var sqlDriverName = null
        //  private var configFileReader: ConfigFileReader = null
  private var dbConnection = "mongodb"
  private var dbDatabase = "trading"
  private var dbPassword = "mongodb"
  private var dbUsername = "trading_user_RW"
  private var bootstrapServers = "rhes75:9092, rhes75:9093, rhes75:9094"
  private var schemaRegistryURL = "http://rhes75:8081"
  private var zookeeperConnect = "rhes75:2181" 
  private var zookeeperConnectionTimeoutMs = "10000"
  private var rebalanceBackoffMS = "15000"
  private var zookeeperSessionTimeOutMs = "15000"
  private var autoCommitIntervalMS = "12000"
  private var topicsValue = "final"
  private var memorySet = "F"
  private var enableHiveSupport = null
  private var enableHiveSupportValue = "true"
  private var sparkStreamingReceiverMaxRateValue = "0" 
  private var checkpointdir = "/checkpoint"
  private var mongodbHost = "rhes75"
  private var mongodbPort = "60100"
  private var zookeeperHost = "rhes75"
  private var zooKeeperClientPort = "2181"
  private var batchInterval = 2


  def main(args: Array[String])
  {
    // Create a StreamingContext with two working thread and batch interval of 2 seconds.

   var startTimeQuery = System.currentTimeMillis

    // Start mongoDB collection stuff
    val collectionName = "MARKETDATAMONGODBSPEED"
    val connectionString = dbConnection+"://"+dbUsername+":"+dbPassword+"@"+mongodbHost+":"+mongodbPort+"/"+dbDatabase+"."+collectionName             
   val sparkConf = new SparkConf().
             setAppName(sparkAppName).
             set("spark.driver.allowMultipleContexts", "true").
             set("spark.hadoop.validateOutputSpecs", "false")

             // change the values accordingly.
             sparkConf.set("sparkDefaultParllelism", sparkDefaultParallelismValue)
             sparkConf.set("sparkSerializer", sparkSerializerValue)
             sparkConf.set("sparkNetworkTimeOut", sparkNetworkTimeOutValue)
             // If you want to see more details of batches please increase the value
             // and that will be shown in UI.
             sparkConf.set("sparkStreamingUiRetainedBatches",
                           sparkStreamingUiRetainedBatchesValue)
             sparkConf.set("sparkWorkerUiRetainedDrivers",
                           sparkWorkerUiRetainedDriversValue)
             sparkConf.set("sparkWorkerUiRetainedExecutors",
                           sparkWorkerUiRetainedExecutorsValue)
             sparkConf.set("sparkWorkerUiRetainedStages",
                           sparkWorkerUiRetainedStagesValue)
             sparkConf.set("sparkUiRetainedJobs", sparkUiRetainedJobsValue)
             sparkConf.set("enableHiveSupport",enableHiveSupportValue)
             if (memorySet == "T")
             {
               sparkConf.set("spark.driver.memory", "18432M")
             }
             sparkConf.set("spark.streaming.stopGracefullyOnShutdown","true")
             sparkConf.set("spark.streaming.receiver.writeAheadLog.enable", "true")
             sparkConf.set("spark.streaming.driver.writeAheadLog.closeFileAfterWrite", "true")
             sparkConf.set("spark.streaming.receiver.writeAheadLog.closeFileAfterWrite", "true")
             sparkConf.set("spark.streaming.backpressure.enabled","true")
             sparkConf.set("spark.streaming.receiver.maxRate",sparkStreamingReceiverMaxRateValue)
	     sparkConf.set("spark.mongodb.input.uri", connectionString)
	     sparkConf.set("spark.mongodb.output.uri", connectionString)

             //  --conf "spark.mongodb.input.uri=mongodb://test_user_RW:mongodb@rhes75:60100/test.dummy_mongoDB" --conf "spark.mongodb.output.uri=mongodb://test_user_RW:mongodb@rhes75:60100/test.dummy_mongoDB"
    val streamingContext = new StreamingContext(sparkConf, Seconds(batchInterval))
    val sparkContext  = streamingContext.sparkContext
    val sqlContext= new org.apache.spark.sql.SQLContext(sparkContext)
    import sqlContext.implicits._
    sparkContext.setLogLevel("ERROR")

       // Start mongoDB collection stuff
    val rdd = MongoSpark.load(sparkContext)
    val MARKETDATAMONGODBSPEED = rdd.toDF
    var rows = 0
    // get No of Docs in MongoDB collections
    rows = MARKETDATAMONGODBSPEED.count.toInt
    println("Documents in " + collectionName + ": " + rows)

    var sqltext = ""
    var totalPrices: Long = 0
    val runTime = 240
    //val HiveContext = new HiveContext(streamingContext.sparkContext)
    
    //streamingContext.checkpoint("checkpointdir")
    val writeConfig = WriteConfig(Map("collection" -> collectionName, "writeConcern.w" -> "majority"), Some(WriteConfig(sparkContext)))
    //val kafkaParams = Map[String, String]("bootstrap.servers" -> bootstrapServers, "schema.registry.url" -> schemaRegistryURL, "zookeeper.connect" -> zookeeperConnect, "group.id" -> sparkAppName )
    val kafkaParams = Map[String, String](
                                      "bootstrap.servers" -> bootstrapServers,
                                      "schema.registry.url" -> schemaRegistryURL,
                                       "zookeeper.connect" -> zookeeperConnect,
                                       "group.id" -> sparkAppName,
                                       "zookeeper.connection.timeout.ms" -> zookeeperConnectionTimeoutMs,
                                       "rebalance.backoff.ms" -> rebalanceBackoffMS,
                                       "zookeeper.session.timeout.ms" -> zookeeperSessionTimeOutMs,
                                       "auto.commit.interval.ms" -> autoCommitIntervalMS
                                     )
    //val topicsSet = topics.split(",").toSet
    val topics = Set(topicsValue)
    val dstream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](streamingContext, kafkaParams, topics)
    dstream.cache()
    val comma = ", "
    val q = """""""
    var current_timestamp = "current_timestamp"
    case class columns(KEY: String, TICKER: String, TIMEISSUED: String, PRICE: Double)
    // Work on every Stream
    dstream.foreachRDD
    { pricesRDD =>
      val x= pricesRDD.count
      // Check if any messages in
      if (x > 0)
      {
         println("Current time is: " + Calendar.getInstance.getTime)
         // Combine each partition's results into a single RDD
         val cachedRDD = pricesRDD.repartition(1).cache
         //cachedRDD.saveAsTextFile("/data/prices/prices_" + System.currentTimeMillis.toString)
         totalPrices += cachedRDD.count
         var endTimeQuery = System.currentTimeMillis
         println("TotalPrices streamed in so far: " +totalPrices+ " , Runnig for  " + (endTimeQuery - startTimeQuery)/(1000*60)+" Minutes")
         // Check if running time > runTime exit
         if( (endTimeQuery - startTimeQuery)/(100000*60) > runTime)
         {
           println("\nDuration exceeded " + runTime + " minutes exiting")
           System.exit(0)
         }

         // Work on individual messages
         for(line <- pricesRDD.collect.toArray)
         {
           var key = line._2.split(',').view(0).toString
           var ticker =  line._2.split(',').view(1).toString
           var timeissued = line._2.split(',').view(2).toString
           var price = line._2.split(',').view(3).toString.toDouble
           var priceToString = line._2.split(',').view(3)
           var CURRENCY = "GBP"
           var op_type = "1"
           var op_time = System.currentTimeMillis.toString
           if (price > 90.0)
           {
             //println ("price > 90.0, saving to MongoDB collection!")

             var document = sparkContext.parallelize((1 to 1).
                            map(i => Document.parse(s"{key:'$key',ticker:'$ticker',timeissued:'$timeissued',price:$price,CURRENCY:'$CURRENCY',op_type:$op_type,op_time:'$op_time'}")))
             //
             // Writing document to MongoDB collection
             //
             MongoSpark.save(document, writeConfig)

             if(ticker == "VOD" && price > 99.0)
             { 
               sqltext = Calendar.getInstance.getTime.toString + ", Price on "+ticker+" hit " +price.toString
               //java.awt.Toolkit.getDefaultToolkit().beep()
               println(sqltext)
             }
           }
         }
      }
    }
    streamingContext.start() 
    streamingContext.awaitTermination() 
    //streamingContext.stop()
  }
}
